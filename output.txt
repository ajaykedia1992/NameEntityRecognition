/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0806 19:14:28.957301 140285714007872 deprecation_wrapper.py:119] From model.py:101: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-08-06 19:14:28.985783: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-06 19:14:29.025433: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-08-06 19:14:29.026213: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45669f0 executing computations on platform Host. Devices:
2019-08-06 19:14:29.026280: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
W0806 19:14:30.160438 140285714007872 deprecation_wrapper.py:119] From model.py:104: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2019-08-06 19:14:30.369108: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
W0806 19:14:30.634490 140285714007872 deprecation_wrapper.py:119] From model.py:105: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

W0806 19:14:30.758688 140285714007872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0806 19:14:30.759138 140285714007872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0806 19:14:36.045646 140285714007872 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0806 19:14:42.105628 140285714007872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0806 19:14:42.371173 140285714007872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Total number of words = 6363
Total number of tags = 20
biggest sentence has 122 words
1919
214
1919
214
1600
1600
512
512
Train on 1600 samples, validate on 512 samples
Epoch 1/3

  32/1600 [..............................] - ETA: 13:01 - loss: 2.9865
  64/1600 [>.............................] - ETA: 9:08 - loss: 2.1436 
  96/1600 [>.............................] - ETA: 7:44 - loss: 1.7027
 128/1600 [=>............................] - ETA: 6:59 - loss: 1.4733
 160/1600 [==>...........................] - ETA: 6:28 - loss: 1.3422
 192/1600 [==>...........................] - ETA: 6:05 - loss: 1.2061
 224/1600 [===>..........................] - ETA: 5:46 - loss: 1.1381
 256/1600 [===>..........................] - ETA: 5:31 - loss: 1.0769
 288/1600 [====>.........................] - ETA: 5:17 - loss: 1.0303
 320/1600 [=====>........................] - ETA: 5:06 - loss: 0.9818
 352/1600 [=====>........................] - ETA: 4:54 - loss: 0.9545
 384/1600 [======>.......................] - ETA: 4:44 - loss: 0.9161
 416/1600 [======>.......................] - ETA: 4:34 - loss: 0.8809
 448/1600 [=======>......................] - ETA: 4:25 - loss: 0.8485
 480/1600 [========>.....................] - ETA: 4:15 - loss: 0.8229
 512/1600 [========>.....................] - ETA: 4:06 - loss: 0.7975
 544/1600 [=========>....................] - ETA: 3:58 - loss: 0.7756
 576/1600 [=========>....................] - ETA: 3:50 - loss: 0.7608
 608/1600 [==========>...................] - ETA: 3:42 - loss: 0.7459
 640/1600 [===========>..................] - ETA: 3:34 - loss: 0.7253
 672/1600 [===========>..................] - ETA: 3:26 - loss: 0.7097
 704/1600 [============>.................] - ETA: 3:18 - loss: 0.6964
 736/1600 [============>.................] - ETA: 3:11 - loss: 0.6836
 768/1600 [=============>................] - ETA: 3:03 - loss: 0.6708
 800/1600 [==============>...............] - ETA: 2:55 - loss: 0.6609
 832/1600 [==============>...............] - ETA: 2:48 - loss: 0.6502
 864/1600 [===============>..............] - ETA: 2:41 - loss: 0.6374
 896/1600 [===============>..............] - ETA: 2:33 - loss: 0.6272
 928/1600 [================>.............] - ETA: 2:26 - loss: 0.6174
 960/1600 [=================>............] - ETA: 2:19 - loss: 0.6113
 992/1600 [=================>............] - ETA: 2:12 - loss: 0.6009
1024/1600 [==================>...........] - ETA: 2:05 - loss: 0.5909
1056/1600 [==================>...........] - ETA: 1:58 - loss: 0.5820
1088/1600 [===================>..........] - ETA: 1:51 - loss: 0.5762
1120/1600 [====================>.........] - ETA: 1:43 - loss: 0.5676
1152/1600 [====================>.........] - ETA: 1:36 - loss: 0.5597
1184/1600 [=====================>........] - ETA: 1:29 - loss: 0.5563
1216/1600 [=====================>........] - ETA: 1:22 - loss: 0.5484
1248/1600 [======================>.......] - ETA: 1:15 - loss: 0.5431
1280/1600 [=======================>......] - ETA: 1:09 - loss: 0.5354
1312/1600 [=======================>......] - ETA: 1:02 - loss: 0.5320
1344/1600 [========================>.....] - ETA: 55s - loss: 0.5253 
1376/1600 [========================>.....] - ETA: 48s - loss: 0.5205
1408/1600 [=========================>....] - ETA: 41s - loss: 0.5149
1440/1600 [==========================>...] - ETA: 34s - loss: 0.5090
1472/1600 [==========================>...] - ETA: 27s - loss: 0.5063
1504/1600 [===========================>..] - ETA: 20s - loss: 0.5025
1536/1600 [===========================>..] - ETA: 13s - loss: 0.4998
1568/1600 [============================>.] - ETA: 6s - loss: 0.4956 
1600/1600 [==============================] - 428s 267ms/step - loss: 0.4899 - val_loss: 0.2399
Epoch 2/3

  32/1600 [..............................] - ETA: 5:25 - loss: 0.3360
  64/1600 [>.............................] - ETA: 5:15 - loss: 0.3109
  96/1600 [>.............................] - ETA: 5:10 - loss: 0.3363
 128/1600 [=>............................] - ETA: 5:06 - loss: 0.3035
 160/1600 [==>...........................] - ETA: 5:00 - loss: 0.2803
 192/1600 [==>...........................] - ETA: 4:54 - loss: 0.2657
 224/1600 [===>..........................] - ETA: 4:48 - loss: 0.2566
 256/1600 [===>..........................] - ETA: 4:40 - loss: 0.2543
 288/1600 [====>.........................] - ETA: 4:33 - loss: 0.2610
 320/1600 [=====>........................] - ETA: 4:27 - loss: 0.2609
 352/1600 [=====>........................] - ETA: 4:20 - loss: 0.2623
 384/1600 [======>.......................] - ETA: 4:13 - loss: 0.2549
 416/1600 [======>.......................] - ETA: 4:07 - loss: 0.2497
 448/1600 [=======>......................] - ETA: 4:00 - loss: 0.2488
 480/1600 [========>.....................] - ETA: 3:53 - loss: 0.2488
 512/1600 [========>.....................] - ETA: 3:47 - loss: 0.2438
 544/1600 [=========>....................] - ETA: 3:40 - loss: 0.2421
 576/1600 [=========>....................] - ETA: 3:33 - loss: 0.2414
 608/1600 [==========>...................] - ETA: 3:26 - loss: 0.2445
 640/1600 [===========>..................] - ETA: 3:20 - loss: 0.2476
 672/1600 [===========>..................] - ETA: 3:13 - loss: 0.2476
 704/1600 [============>.................] - ETA: 3:07 - loss: 0.2466
 736/1600 [============>.................] - ETA: 3:00 - loss: 0.2437
 768/1600 [=============>................] - ETA: 2:53 - loss: 0.2442
 800/1600 [==============>...............] - ETA: 2:47 - loss: 0.2399
 832/1600 [==============>...............] - ETA: 2:40 - loss: 0.2405
 864/1600 [===============>..............] - ETA: 2:33 - loss: 0.2371
 896/1600 [===============>..............] - ETA: 2:26 - loss: 0.2338
 928/1600 [================>.............] - ETA: 2:20 - loss: 0.2312
 960/1600 [=================>............] - ETA: 2:13 - loss: 0.2306
 992/1600 [=================>............] - ETA: 2:06 - loss: 0.2301
1024/1600 [==================>...........] - ETA: 2:00 - loss: 0.2271
1056/1600 [==================>...........] - ETA: 1:53 - loss: 0.2263
1088/1600 [===================>..........] - ETA: 1:46 - loss: 0.2240
1120/1600 [====================>.........] - ETA: 1:40 - loss: 0.2233
1152/1600 [====================>.........] - ETA: 1:33 - loss: 0.2229
1184/1600 [=====================>........] - ETA: 1:26 - loss: 0.2209
1216/1600 [=====================>........] - ETA: 1:20 - loss: 0.2191
1248/1600 [======================>.......] - ETA: 1:13 - loss: 0.2194
1280/1600 [=======================>......] - ETA: 1:06 - loss: 0.2191
1312/1600 [=======================>......] - ETA: 1:00 - loss: 0.2197
1344/1600 [========================>.....] - ETA: 53s - loss: 0.2178 
1376/1600 [========================>.....] - ETA: 46s - loss: 0.2181
1408/1600 [=========================>....] - ETA: 40s - loss: 0.2155
1440/1600 [==========================>...] - ETA: 33s - loss: 0.2150
1472/1600 [==========================>...] - ETA: 26s - loss: 0.2146
1504/1600 [===========================>..] - ETA: 20s - loss: 0.2127
1536/1600 [===========================>..] - ETA: 13s - loss: 0.2112
1568/1600 [============================>.] - ETA: 6s - loss: 0.2105 
1600/1600 [==============================] - 418s 261ms/step - loss: 0.2104 - val_loss: 0.1733
Epoch 3/3

  32/1600 [..............................] - ETA: 5:28 - loss: 0.2121
  64/1600 [>.............................] - ETA: 5:21 - loss: 0.1934
  96/1600 [>.............................] - ETA: 5:14 - loss: 0.1695
 128/1600 [=>............................] - ETA: 5:06 - loss: 0.1900
 160/1600 [==>...........................] - ETA: 4:59 - loss: 0.1907
 192/1600 [==>...........................] - ETA: 4:52 - loss: 0.1797
 224/1600 [===>..........................] - ETA: 4:47 - loss: 0.1783
 256/1600 [===>..........................] - ETA: 4:39 - loss: 0.1766
 288/1600 [====>.........................] - ETA: 4:32 - loss: 0.1755
 320/1600 [=====>........................] - ETA: 4:26 - loss: 0.1786
 352/1600 [=====>........................] - ETA: 4:19 - loss: 0.1771
 384/1600 [======>.......................] - ETA: 4:11 - loss: 0.1751
 416/1600 [======>.......................] - ETA: 4:05 - loss: 0.1736
 448/1600 [=======>......................] - ETA: 3:59 - loss: 0.1692
 480/1600 [========>.....................] - ETA: 3:52 - loss: 0.1693
 512/1600 [========>.....................] - ETA: 3:45 - loss: 0.1669
 544/1600 [=========>....................] - ETA: 3:39 - loss: 0.1684
 576/1600 [=========>....................] - ETA: 3:32 - loss: 0.1684
 608/1600 [==========>...................] - ETA: 3:26 - loss: 0.1671
 640/1600 [===========>..................] - ETA: 3:19 - loss: 0.1633
 672/1600 [===========>..................] - ETA: 3:12 - loss: 0.1623
 704/1600 [============>.................] - ETA: 3:06 - loss: 0.1631
 736/1600 [============>.................] - ETA: 2:59 - loss: 0.1628
 768/1600 [=============>................] - ETA: 2:53 - loss: 0.1605
 800/1600 [==============>...............] - ETA: 2:46 - loss: 0.1608
 832/1600 [==============>...............] - ETA: 2:40 - loss: 0.1597
 864/1600 [===============>..............] - ETA: 2:33 - loss: 0.1592
 896/1600 [===============>..............] - ETA: 2:26 - loss: 0.1573
 928/1600 [================>.............] - ETA: 2:20 - loss: 0.1565
 960/1600 [=================>............] - ETA: 2:13 - loss: 0.1580
 992/1600 [=================>............] - ETA: 2:06 - loss: 0.1574
1024/1600 [==================>...........] - ETA: 2:00 - loss: 0.1570
1056/1600 [==================>...........] - ETA: 1:53 - loss: 0.1566
1088/1600 [===================>..........] - ETA: 1:46 - loss: 0.1561
1120/1600 [====================>.........] - ETA: 1:40 - loss: 0.1562
1152/1600 [====================>.........] - ETA: 1:33 - loss: 0.1557
1184/1600 [=====================>........] - ETA: 1:26 - loss: 0.1590
1216/1600 [=====================>........] - ETA: 1:20 - loss: 0.1583
1248/1600 [======================>.......] - ETA: 1:13 - loss: 0.1580
1280/1600 [=======================>......] - ETA: 1:06 - loss: 0.1575
1312/1600 [=======================>......] - ETA: 1:00 - loss: 0.1577
1344/1600 [========================>.....] - ETA: 53s - loss: 0.1575 
1376/1600 [========================>.....] - ETA: 46s - loss: 0.1565
1408/1600 [=========================>....] - ETA: 40s - loss: 0.1558
1440/1600 [==========================>...] - ETA: 33s - loss: 0.1564
1472/1600 [==========================>...] - ETA: 26s - loss: 0.1565
1504/1600 [===========================>..] - ETA: 20s - loss: 0.1567
1536/1600 [===========================>..] - ETA: 13s - loss: 0.1572
1568/1600 [============================>.] - ETA: 6s - loss: 0.1581 
1600/1600 [==============================] - 417s 261ms/step - loss: 0.1573 - val_loss: 0.1440

 32/192 [====>.........................] - ETA: 37s
 64/192 [=========>....................] - ETA: 25s
 96/192 [==============>...............] - ETA: 18s
128/192 [===================>..........] - ETA: 11s
160/192 [========================>.....] - ETA: 5s 
192/192 [==============================] - 34s 175ms/step
                                 precision    recall  f1-score   support

                  Biotic_Entity       0.78      0.87      0.82       425
                    Eventuality       0.77      0.81      0.79       410
                        Quality       0.69      0.41      0.52       146
                          Value       0.59      0.67      0.62        15
Aggregate_Biotic_Abiotic_Entity       0.71      0.74      0.73       133
                 Abiotic_Entity       0.74      0.88      0.80        57
                           Unit       1.00      0.67      0.80         9
                           Time       0.58      0.51      0.54        37
                       Location       0.86      0.60      0.71        10

                      micro avg       0.75      0.77      0.76      1242
                      macro avg       0.75      0.77      0.75      1242

